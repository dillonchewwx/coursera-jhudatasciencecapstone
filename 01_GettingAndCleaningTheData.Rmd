---
title: "Task 1 - Getting and cleaning the data"
author: "dillonchewwx"
date: "25/06/2021"
output:
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
---
# Task 1 - Getting and cleaning the data

The goal of this task is to get familiar with the databases and do the necessary cleaning. Note that the data do contain words of offensive and profane meaning which are left there intentionally to highlight the fact that the developer has to work on them. As stated in the task sheet, the two key tasks are to:

1. Tokenization - identifying appropriate tokens such as words, punctuation, and numbers. 
2. Profanity filtering - removing profanity and other words you do not want to predict.

# Load libraries
```{r Load libraries, message=FALSE, results='hide', warning=FALSE}
library(knitr)
library(tidyverse)
library(tm)
```

# Load raw data
```{r Load the Data, warning=FALSE}
blogs_file<-"../Data/en_US.blogs.txt"
news_file<-"../Data/en_US.news.txt"
twitter_file<-"../Data/en_US.twitter.txt"

blogs<-read_lines(blogs_file, skip_empty_rows=TRUE)
news<-read_lines(news_file, skip_empty_rows=TRUE)
twitter<-read_lines(twitter_file, skip_empty_rows=TRUE)
```

# Preliminary statistics of the data
```{r Prelim Stats}
data_summary<-tibble(`Data Set` = c("Blogs", "News", "Twitter"),
                     `File Size (MB)` = c(file.size(blogs_file)/1e6, file.size(news_file)/1e6, file.size(twitter_file)/1e6),
                     `No. of Lines` = c(length(blogs), length(news), length(twitter)), 
                     `No. of Words` = c(sum(str_count(blogs, "\\S+")), sum(str_count(news, "\\S+")), sum(str_count(twitter, "\\S+"))), 
                     `No. of Characters` = c(sum(nchar(blogs)), sum(nchar(news)), sum(nchar(twitter))))
kable(data_summary)
```

# Sample the data
In this step, we will randomly select rows to get a sense of what kind of text is in the data. This will then allow us to create specific filters for cleaning up the data. Here, we will use the `sample` function to select 10% of the data for viewing.
```{r Sample Data}
set.seed(123)
blogs_sample<-sample(blogs, length(blogs)*0.1)
news_sample<-sample(news, length(news)*0.1)
twitter_sample<-sample(twitter, length(twitter)*0.1)

head(blogs_sample,30)
head(news_sample,30)
head(twitter_sample,30)
```
From this sampled set, some key observations include the following which has to be removed:

1. Presence of many different punctuation 
2. Smiley faces in twitter data set.
3. Backslash character indicating new lines in the news data set 
4. URLs

# Clean the data
In this step, we will clean up the data by applying the following pre-processing steps:

1. Remove URLs
2. Convert all letters to lower case
3. Remove any numbers
4. Remove all punctuation
5. Remove white space

Following which, we will create a Corpus with the `tm` package and apply the two transformations:
1. Remove profanities: [List of Offensive/Profane words](https://www.cs.cmu.edu/~biglou/resources/bad-words.txt)
2. Remove stop words
```{r Clean Data}
combined_data<-c(blogs, news, twitter) %>%
    gsub("http[^[:space:]]*", "", .) %>%
    str_to_lower() %>%
    gsub("[0-9]", "", .) %>%
    removePunctuation() %>%
    stripWhitespace()

profanities<-read_tsv("https://www.cs.cmu.edu/~biglou/resources/bad-words.txt")

combined_Corpus<-Corpus(VectorSource(combined_data)) %>%
    tm_map(removeWords, stopwords()) %>%
    tm_map(removeWords, profanities$X1)

as.character(combined_Corpus[1])
```