---
title: "Milestone Report"
subtitle: "JHU Data Science Specialization Capstone Project"
author: "dillonchewwx"
date: "`r Sys.Date()`"
output: 
    rmdformats::robobook
---
## Overview

In this capstone project, data science would be applied in the area of natural language processing where the end goal would be to understand and build predictive text models. 

For this milestone report, the findings from the exploratory analysis will be summarized and a simple model for the relationship between words will be built. An [n-gram model](http://en.wikipedia.org/wiki/N-gram) would be trained from [three sources of text data](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) in English to predict the next word based on the previous 1, 2 or 3 words. 

## Environment Setup
Here, the session is initialized by loading relevant packages. 
```{r LoadLibraries, warning=FALSE, message=FALSE}
library(knitr)
library(scales)
library(tidytext)
library(tidyverse)
library(wordcloud)
```

## Load the data
The data has been pre-downloaded, and saved into a folder called `Data`. In addition, a list of [profanities]("https://www.cs.cmu.edu/~biglou/resources/bad-words.txt") in English was downloaded and would be used for subsequent filtering steps.  
```{r LoadData, warning=FALSE, message=FALSE}
blogs_file<-"../Data/en_US.blogs.txt"
news_file<-"../Data/en_US.news.txt"
twitter_file<-"../Data/en_US.twitter.txt"

blogs<-read_lines(blogs_file, skip_empty_rows=TRUE) %>% 
  tibble() %>% 
  rename(text=colnames(.)) %>%
  mutate(source="blogs")
twitter<-read_lines(twitter_file, skip_empty_rows=TRUE) %>% 
  tibble() %>% 
  rename(text=colnames(.)) %>%
  mutate(source="twitter")
news<-read_lines(news_file, skip_empty_rows=TRUE) %>% 
  tibble() %>%
  rename(text=colnames(.)) %>%
  mutate(source="news")

profanities<-read_tsv("https://www.cs.cmu.edu/~biglou/resources/bad-words.txt", col_names=FALSE) %>%
  rename(word=colnames(.))
```

# Preliminary statistics of the data
Before carrying out any tokenization, a basic summary of the three text sources will be calculated. 
```{r Prelim Stats}
data_summary<-tibble(`Data Set` = c("Blogs", "News", "Twitter"),
                     `File Size (MB)` = c(file.size(blogs_file)/1e6, 
                                          file.size(news_file)/1e6,
                                          file.size(twitter_file)/1e6),
                     `No. of Lines` = c(tally(blogs), tally(news), tally(twitter)), 
                     `No. of Words` = c(sum(str_count(blogs$text, "\\S+")), 
                                        sum(str_count(news$text, "\\S+")), 
                                        sum(str_count(twitter$text, "\\S+"))), 
                     `No. of Characters` = c(sum(nchar(blogs$text)), sum(nchar(news$text)), sum(nchar(twitter$text)))
                     )
kable(data_summary, digits=3, format.args=list(big.mark = ","))
```

Here, it is noted that the files are generally quite large and thus to improve the processing time, sampling of the data would have to be done for subsequent analyses. 

# Sample and Clean the Data
In this step, 10% of the rows will be randomly selected using the `sample` function to get a non-biased sample of the data. 

```{r Sample Data}
set.seed(123)
blogs_sample<-slice_sample(blogs, prop=0.1)
news_sample<-slice_sample(news, prop=0.1)
twitter_sample<-slice_sample(twitter, prop=0.1)
```

Following which, the data will be cleaned up by carrying out a series of transformation steps before breaking the text up into individual tokens with `unnest_tokens()` function from the `tidytext` package. Note that the `unnest_tokens()` function by default strips all punctuation and converts the tokens to lowercase. 

1. Remove URLs, handles, hashtags and emails
2. Remove numbers
3. Remove profanities

```{r Clean Data, message=FALSE}
combined<-blogs_sample %>%
  full_join(news_sample) %>%
  full_join(twitter_sample)
  
cleaned_combined<-combined %>%
  mutate(text=str_replace_all(text, "(www|http:|https:)+[^\\s]+[\\w]", "")) %>% # Remove URLs
  mutate(text=str_replace_all(text, "@[^\\s]+", "")) %>% # Remove handles
  mutate(text=str_replace_all(text, "#[^\\s]+", "")) %>% # Remove hashtags
  mutate(text=str_replace_all(text, "\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b", "")) %>% # Remove emails
  mutate(text=str_replace_all(text, "[[:digit:]]+", "")) # Remove numbers
  
unigrams<-cleaned_combined %>%
  unnest_tokens(word, text) %>%
  anti_join(profanities) # Remove profanities

glimpse(unigrams)
```

The data is now in a one-word-per-row format and can be manipulated with tidyverse tools. Note that as the goal of this project is to build a predictive text model, common English stop words would not be removed. 
```{r Test profanity filter}
# Test profanity filter
str_detect(unigrams$word, "fuck") %>% sum()
str_detect(unigrams$word, "cunt") %>% sum()
unigrams$word[str_which(unigrams$word, "cunt")]
```

It seems that the list of 1.38k profanities were not enough to cover all the profanities and thus perhaps a more comprehensive list could be used in the future.

# Exploratory Data Analysis
For this section, exploratory data analysis would be conducted to understand the distribution and relationship between the words, tokens, and phrases in the text.

## Word Frequencies
```{r Word Frequency}
word_frequency<-unigrams %>%
  group_by(source) %>%
  count(word, sort=TRUE)

ggplot(word_frequency %>% slice_head(n=20), aes(x=n, y=reorder(word, n), fill=source)) +
  geom_bar(stat="identity") + 
  facet_wrap(~source, scales="free") + 
  theme_bw() + 
  labs(x="count", y="word") + 
  scale_x_continuous(labels=comma_format()) +
  theme(legend.position="none")

# For combined word cloud
combined_word_frequency<-unigrams %>% 
  select(word) %>%
  count(word, sort=TRUE)
  
wordcloud(words=combined_word_frequency$word, freq=combined_word_frequency$n, max.words=150,
          scale=c(3,1), random.order=FALSE, 
          colors=brewer.pal(8, "Dark2"), rot.per=0.1)
```

From the bar charts, it is observed that the top five most common words ("the", "to", "and", "a", "of") used across the three sources are consistent. Likewise, a similar trend was observed when the three sources were combined. 

## Unique words coverage

```{r 50% Word Coverage}
coverage_50<-combined_word_frequency %>%
  mutate(prop=n/sum(n), 
         coverage=cumsum(prop)) %>%
  filter(coverage<=0.5)
nrow(coverage_50)
```

It is observed that 50% of all words are covered by just `r nrow(coverage_50)` words.

```{r 90% Word Coverage}
coverage_90<-combined_word_frequency %>%
  mutate(prop=n/sum(n), 
         coverage=cumsum(prop)) %>%
  filter(coverage<=0.9)
nrow(coverage_90)
```

To cover 90% of all words, `r nrow(coverage_90)` words are required. 

# N-gram models 

So far, the words have been considered as individual units. However, many interesting text analyses are based on the relationships between words, such as examining which words tend to follow others immediately, or that tend to co-occur within the same documents. Here, 2-grams, 3-grams, and 4-grams would be examined. 

## Bigrams

```{r Bigrams}
bigrams<-cleaned_combined %>%
  unnest_tokens(bigram, text, token="ngrams", n=2)

bigrams_filtered<-bigrams %>%
  separate(bigram, into=c("word1", "word2"), sep=" ") %>%
  filter(!word1 %in% profanities$word) %>%
  filter(!word2 %in% profanities$word) %>%
  group_by(source) %>%
  unite(bigram, word1, word2, sep=" ") %>%
  count(bigram, sort=TRUE)

ggplot(bigrams_filtered %>% slice_head(n=20), aes(x=n, y=reorder(bigram, n), fill=source)) +
  geom_bar(stat="identity") + 
  theme_bw() + 
  facet_wrap(~source, scales="free") +
  labs(x="count", y="bigram") + 
  scale_x_continuous(labels=comma_format()) +
  theme(legend.position="none")
```

## Trigrams

```{r Trigrams}
trigrams<-cleaned_combined %>%
  unnest_tokens(trigram, text, token="ngrams", n=3)

trigrams_filtered<-trigrams %>%
  separate(trigram, c("word1", "word2", "word3"), sep=" ") %>%
  filter(!word1 %in% profanities$word) %>%
  filter(!word2 %in% profanities$word) %>%
  filter(!word3 %in% profanities$word) %>%
  group_by(source) %>%
  unite(trigram, word1, word2, word3, sep=" ") %>%
  count(trigram, sort=TRUE)

ggplot(trigrams_filtered %>% group_by(source) %>% filter(!trigram == "NA NA NA") %>% slice_head(n=20), aes(x=n, y=reorder(trigram, n), fill=source)) +
  geom_bar(stat="identity") + 
  facet_wrap(~source, scales="free") + 
  theme_bw() + 
  labs(x="count", y="trigram") + 
  scale_x_continuous(labels=comma_format()) +
  theme(legend.position="none")
```

## Quadrigrams

```{r Quadrigrams}
quadrigrams<-cleaned_combined %>%
  unnest_tokens(quadrigram, text, token="ngrams", n=4)

quadrigrams_filtered<-quadrigrams %>%
  separate(quadrigram, c("word1", "word2", "word3", "word4"), sep=" ") %>%
  filter(!word1 %in% profanities$word) %>%
  filter(!word2 %in% profanities$word) %>%
  filter(!word3 %in% profanities$word) %>%
  filter(!word4 %in% profanities$word) %>%
  group_by(source) %>%
  unite(quadrigram, word1, word2, word3, word4, sep=" ") %>%
  count(quadrigram, sort=TRUE)

ggplot(quadrigrams_filtered %>% group_by(source) %>% filter(!quadrigram == "NA NA NA NA") %>% slice_head(n=20), aes(x=n, y=reorder(quadrigram, n), fill=source)) +
  geom_bar(stat="identity") + 
  facet_wrap(~source, scales="free") + 
  theme_bw() + 
  labs(x="count", y="quadrigram") + 
  scale_x_continuous(labels=comma_format()) +
  theme(legend.position="none")
```

From these graphs, we note that the bigrams and trigrams across the three sources are largely similar, but the quadrigrams are much more varied. 

# Next steps

Using the n-gram models, a predictive algorithm would be developed to predict the next word based on an input phrase of one to three words. Possible strategies are as follow:

1. Find the highest frequency n+1-gram with the input phrase.
2. Start with the quadrigram model, then trigram and bigram. 

The final strategy used would be based on the accuracy and efficiency which would be evaluated. Lastly, for the final deliverable, a Shiny app would be created which would take an input phrase and provide the prediction output of the next word. 