---
title: "Task 0 - Understanding The Problem"
author: "dillonchewwx"
date: "14/06/2021"
output:
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
---
# Task 0 - Understanding the Problem

The first step in analyzing any new data set is figuring out: (a) what data you have and (b) what are the standard tools and models used for that type of data. For this project, the data is from a corpus called HC Corpora and can be downloaded from the [Coursera Site](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).  

In this capstone we will be applying data science in the area of natural language processing. As a first step toward working on this project, the first task would be to: 

1. Obtain the data - Can you download the data and load/manipulate it in R?
2. Familiarize with NLP and text mining - Learn about the basics of natural language processing and how it relates to the data science process you have learned in the Data Science Specialization.

# Obtaining the Data

The data can be directly downloaded using the link above. For this project, we will store the data outside of the project folder in a folder called Data. Following which, we will unzip the file and only pull out the English database (en_US). The files can then be read into R for processing. 
```{r Load the Data, echo=FALSE, warning=FALSE}
blogs_file<-"../Data/en_US.blogs.txt"
twitter_file<-"../Data/en_US.twitter.txt"
news_file<-"../Data/en_US.news.txt"

library(tidyverse)
blogs<-read_lines(blogs_file, skip_empty_rows=TRUE)
twitter<-read_lines(twitter_file, skip_empty_rows=TRUE)
news<-read_lines(news_file, skip_empty_rows=TRUE)
```
Let's now take a quick look at the dataset.
```{r}
glimpse(blogs)
mean(nchar(blogs))
```
Here, we see that the blogs dataset consists of almost 900,000 lines of text with each line having an average of `r round(mean(nchar(blogs)),2)` characters.
```{r}
glimpse(twitter)
mean(nchar(twitter))
```
Likewise, for the twitter dataset, we have over 2.3 million lines with an average of `r round(mean(nchar(twitter)),2)` characters in each line. 
```{r}
glimpse(news)
mean(nchar(news))
```
Lastly, the news dataset has about 1 million lines with an average of `r round(mean(nchar(news)),2)` characters in each line.

# Understanding the NLP workflow

For this project, we will adopt the workflow as developed by Feinerer et al. in [	10.18637/jss.v025.i05]( https://www.jstatsoft.org/article/view/v025i05). Here the `tm` package would be used where a brief workflow would be as follow to obtain the Document Term Matrix: 

1. Data import: This involves loading the data into a `Corpus` as defined by the `tm` package. 
2. Pre-processing - Stemming: This step erases word suffixes to reduce the complexity without any severe loss of information. The `stemDoc()` function would be helpful here.
3. Pre-processing - White Space elimination and lower case conversion: This is as described and can be accomplished with the `stripWhitespace()` and `tmTolower()` functions from the `tm` package. Alternative commands are `gsub` and `tolower` as found in base R. 
4. Pre-processing - Stopword removal: these are words so common in language that their information value is almost zero such as I, and, which, won't, for etc... The `tm` package has a default set of stopwords in `stopwords()`, however, we can also define our set of stopwords and remove them using the `removeWords(text, myStopWords)` function.
5. Pre-processing - Synonyms: in some cases, replacing all synonyms by a single word can help to reduce the complexity in the data set while retaining the meaning. However, it is important to note that a word can have several meaning and thus this step is dependent on the question we are trying to answer with the data set. 
6. Create Document Term Matrix: A Document Term Matrix is a matrix with rows representing each line from the data set, and each row indicating the counts of each word in each line. Following this, minor filtering can be done by finding Frequent Terms using `findFreqTerms()` or removing sparse terms with `removeSparseTerms`.

With the Document Term Matrix, we can then conduct exploratory data analysis, modeling and prediction which would make up the core of the project. 
