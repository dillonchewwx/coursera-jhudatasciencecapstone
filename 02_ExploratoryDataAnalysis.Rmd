---
title: "Task 2 - Exploratory Data Analysis"
author: "dillonchewwx"
date: "01/07/2021"
output:
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
---
# Task 2 - Exploratory Data Analysis

The first step in building a predictive model for text is understanding the distribution and relationship between the words, tokens, and phrases in the text. The goal of this task is to understand the basic relationships you observe in the data and prepare to build your first linguistic models.

Tasks to accomplish:

1. Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora. 
2. Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.

# Load libraries

```{r Load libraries, warning=FALSE, message=FALSE}
library(tidyverse)
library(tm)
library(wordcloud)
```

# Load Corpus and convert into Document Term Matrix
```{r Load Corpus}
combined_Corpus<-readRDS("../Data/combined_Corpus.rds")
dtm<-DocumentTermMatrix(combined_Corpus)
inspect(dtm)
```

We note that there are about 4.27M documents and 938k terms - which is a lot of terms. In addition, the sparsity is 100% and thus the Document Term Matrix would be large. Thus, we will follow the [tidytext](https://www.tidytextmining.com/) approach to handle the large sparse Document Term Matrix, which removes all non-zero values.

# Distribution of word frequency
```{r Convert DTM into tidy format}
library(tidytext)

tidyDF<-tidy(dtm) %>%
    count(term, sort=TRUE)
glimpse(tidyDF)
```

The top five most common words across the 3 data sets are `r tidyDF$term[1]`, `r tidyDF$term[2]`, `r tidyDF$term[3]`, `r tidyDF$term[4]`, and `r tidyDF$term[5]`. Now let's calculate the frequency for each word. 

```{r Term Frequency}
frequency<-tidyDF %>%
    mutate(frequency = n/sum(n))
glimpse(frequency)
```

Likewise, we can make some plots to visualize the data.

```{r Frequency Plots}
ggplot(frequency %>% slice_head(n=20), aes(x=reorder(term, -frequency), y=frequency)) +
    geom_bar(stat="identity", fill="turquoise4", color="black") +
    theme_bw() +
    labs(x="word", y="proportion")

set.seed(123)
wordcloud(words=frequency$term, freq=frequency$n, max.words=150,
          scale=c(3,0.5), random.order=FALSE, 
          colors=brewer.pal(8, "Dark2"), rot.per=0.25)
```

# Unique words to cover 50% and 90% of all word instances
```{r Unique words coverage}
coverage_50<-frequency %>% 
  mutate(coverage=cumsum(frequency)) %>%
  filter(coverage<=0.5)
nrow(coverage_50)
coverage_90<-frequency %>% 
  mutate(coverage=cumsum(frequency)) %>%
  filter(coverage<=0.9)
nrow(coverage_90)
```

Here, we we see that 50% of all words are covered by just `r nrow(coverage_50)` words, while 90% of all words are covered by `r nrow(coverage_90)` words.  

# Frequencies of 2-grams and 3-grams

So far, we have been considering words as individual units. However, many interesting text analyses are based on the relationships between words, whether examining which words tend to follow others immediately, or that tend to co-occur within the same documents. Here, we will use the `unnest_tokens` function from the `tidytext` package to tokenize into consecutive sequences of words. As we cannot use the `tm` package to carry out the same function, we cannot use the previously created Corpus and thus have to reload and clean up all the data before carrying out the analysis.
```{r Load and clean data, warning=FALSE}
blogs_file<-"../Data/en_US.blogs.txt"
news_file<-"../Data/en_US.news.txt"
twitter_file<-"../Data/en_US.twitter.txt"

blogs<-read_lines(blogs_file, skip_empty_rows=TRUE) 
news<-read_lines(news_file, skip_empty_rows=TRUE) 
twitter<-read_lines(twitter_file, skip_empty_rows=TRUE) 

profanities<-read_tsv("https://www.cs.cmu.edu/~biglou/resources/bad-words.txt", col_names=FALSE) %>%
  rename(word=X1)

combined<-c(blogs, news, twitter) %>%
    gsub("http[^[:space:]]*", "", .) %>%
    str_to_lower() %>%
    gsub("[0-9]", "", .) %>%
    removePunctuation(ucp=TRUE) %>%
    stripWhitespace() %>%
    removeWords(profanities$word) %>%
    as.tibble()

head(combined, 10)
```

The cleaned up data is now stored in the `combined` variable and can be used for further processing. We shall now examine the 2-grams. 
```{r 2-grams}
bigrams<-combined %>%
  unnest_tokens(bigram, value, token="ngrams", n=2)

bigrams_frequency<-bigrams %>%
  count(bigram, sort=TRUE) %>% 
  mutate(frequency = n/sum(n))

bigrams_coverage_50<-bigrams_frequency %>% 
  mutate(coverage=cumsum(frequency)) %>%
  filter(coverage<=0.5)
nrow(bigrams_coverage_50)

bigrams_coverage_90<-bigrams_frequency %>%
  mutate(coverage=cumsum(frequency)) %>%
  filter(coverage<=0.9)
nrow(bigrams_coverage_90)

ggplot(bigrams_frequency %>% slice_head(n=20), aes(x=reorder(bigram, frequency), y=frequency)) +
    geom_bar(stat="identity", fill="turquoise4", color="black") +
    theme_bw() +
    labs(x="bigram", y="proportion") +
    coord_flip()
```

For the 2-grams, we can see that 50% of all words are covered by just `r nrow(bigrams_coverage_50)` pairs, while 90% of all words are covered by `r nrow(bigrams_coverage_90)` words. As for the distribution, we can see that the top few 2-grams largely consist of stop words such as "the". We shall now take a look a the 3-grams.

```{r 3-grams}
trigrams<-combined %>%
  unnest_tokens(trigram, value, token="ngrams", n=3)

trigrams_frequency<-trigrams %>%
  count(trigram, sort=TRUE) %>% 
  mutate(frequency = n/sum(n))

trigrams_coverage_50<-trigrams_frequency %>% 
  mutate(coverage=cumsum(frequency)) %>%
  filter(coverage<=0.5)
nrow(trigrams_coverage_50)

trigrams_coverage_90<-trigrams_frequency %>%
  mutate(coverage=cumsum(frequency)) %>%
  filter(coverage<=0.9)
nrow(trigrams_coverage_90)

ggplot(trigrams_frequency %>% slice_head(n=20), aes(x=reorder(trigram, frequency), y=frequency)) +
    geom_bar(stat="identity", fill="turquoise4", color="black") +
    theme_bw() +
    labs(x="trigram", y="proportion") +
    coord_flip()
```

Lastly for the 3-grams, we see that they are much more diverse with over 3 million making up 50% coverage, and 35 million for 90% coverage.
